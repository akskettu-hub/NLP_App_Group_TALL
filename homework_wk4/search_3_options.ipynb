{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b31e5-1f91-434b-97f5-d30ab04e8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # for tfidf functions later\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "## Dependencies for semantic/neural search:\n",
    "import numpy as np\n",
    "!pip install sentence-transformers\n",
    "# We use a pretrained model from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # We can change it to a better model if we find one\n",
    "\n",
    "# neural search function\n",
    "def neural_search(documents, user_input):\n",
    "    \n",
    "    doc_embeddings = model.encode(documents)\n",
    "    query_embedding = model.encode(user_input) # document and query embeddings\n",
    "    \n",
    "    cosine_similarities = np.dot(query_embedding, doc_embeddings.T)# Calculate cosine similarities\n",
    "    \n",
    "    ranked_doc_indices = np.argsort(cosine_similarities)[::-1]  # Rank hits (higher is better), Sort descending\n",
    "    \n",
    "    # Output the results (top 3 matches)\n",
    "    num_results = min(3, len(documents))  # Limit to top 3 results\n",
    "    print(f'Your query \"{user_input}\" matches {len(documents)} documents.')\n",
    "    print(f\"Here are the top {num_results} results:\")\n",
    "    \n",
    "    for i in range(num_results):\n",
    "        doc_idx = ranked_doc_indices[i]        \n",
    "        print(f\"Doc #{i+1} (score: {cosine_similarities[doc_idx]:.4f}): {documents[doc_idx]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Boolean search: Operators\n",
    "\n",
    "d = {\"and\": \"&\", \"AND\": \"&\",\n",
    "     \"or\": \"|\", \"OR\": \"|\",\n",
    "     \"not\": \"1 -\", \"NOT\": \"1 -\",\n",
    "     \"(\": \"(\", \")\": \")\"}          # operator replacements\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# d. Wildcard searches: Let the users search on incomplete terms, such as hous* (easiest) or *ing (similar to previous case) or h*ing (hardest). Read Chapter 3 of the book to learn more about this topic.\n",
    "### comment: I get a \"nothing to repeat at position 0\" error when running this function \n",
    "### Comment, by Akseli: wildcard() returns new string with regex in place of wildcard. The error was either a bug or some extra escape characters were needed. It also appears that the replcae argument needs to be passed as a regex. Also added conditions, because the lack of them caused errors. Should work better now. \n",
    "def wildcard(string:str):\n",
    "    if \"*\" in string:\n",
    "        new_string = re.sub(r\"\\*\", r\"\\\\w*\", string)\n",
    "    if \"?\" in string:\n",
    "        new_string = re.sub(r\"\\?\", r\"\\\\w{1}\", string)\n",
    "    return new_string\n",
    "    \n",
    "# wildcards = {\"*\": r\"\\w*\", \"?\": r\".{1}\"}  # wildcard replacements: \\w* = word characters zero or more times, .{1} = any character exactly once\n",
    "\n",
    "# 4 The most likely reason why not all words are indexed is the default token pattern used by CountVectorizer: r'\\b\\w\\w+\\b' This pattern only matches words with two or more alphanumeric characters. See changed token_pattern when initalizing CountVectorizer at the top of the page\n",
    "\n",
    "# 5\n",
    "def extract_wiki_articles(file: str): \n",
    "    \"\"\"returns a list of strings\"\"\"\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "    articles = text.split(\"</article>\")\n",
    "    cleaned_articles = []\n",
    "    for article in articles:\n",
    "        \n",
    "        cleaned_articles.append(re.sub(r\"<.*>\", \"\", article))\n",
    "        \n",
    "    return cleaned_articles\n",
    "\n",
    "# 2a&b (Stemming & exact matches) \n",
    "# NEW: Two Document Setup Functions\n",
    "\n",
    "### document setup with CountVectorizer\n",
    "# setup index with stemming (for tokens not enclosed in quotes)\n",
    "\n",
    "def document_setup_stem(documents: list):\n",
    "    cv = CountVectorizer(lowercase=True, tokenizer=stem_tokenizer, token_pattern=None, binary=True)\n",
    "    sparse_matrix = cv.fit_transform(documents)\n",
    "    dense_matrix = sparse_matrix.todense()\n",
    "    td_matrix = dense_matrix.T  # rows represent tokens\n",
    "    t2i = cv.vocabulary_\n",
    "    return td_matrix, t2i\n",
    "\n",
    "# setup index without stemming (for exact-match tokens in quotes)\n",
    "def document_setup_exact(documents: list):\n",
    "    cv = CountVectorizer(lowercase=True, token_pattern=r'\\b\\w+\\b', binary=True)\n",
    "    sparse_matrix = cv.fit_transform(documents)\n",
    "    dense_matrix = sparse_matrix.todense()\n",
    "    td_matrix = dense_matrix.T  # rows represent tokens\n",
    "    t2i = cv.vocabulary_\n",
    "    return td_matrix, t2i\n",
    "\n",
    "# tokenizer that applies stemming\n",
    "def stem_tokenizer(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "### document setup with TfIdfVectorizer > not sure whether it can be this simple? \n",
    "def get_tfidf(documents: list):\n",
    "    \"\"\"Returns a tuple with the TfIdf matrix and the vocabulary\"\"\"\n",
    "    tfv = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=None) # logarithmic word frequency, idf, no normalization\n",
    "    matrix = tfv.fit_transform(documents).todense().T\n",
    "    return matrix, tfv.vocabulary_\n",
    "\n",
    "def user_query():\n",
    "    print()\n",
    "    print(\"Type '-r' to search by relevance.\")\n",
    "    print(\"Type '-n' to use neural search.\")\n",
    "    user_input = input(\"Please Enter your query, type 'quit' to exit: \")\n",
    "    return user_input\n",
    "\n",
    "def input_checker(user_input):\n",
    "    if user_input == \"quit\" or user_input == \"\":\n",
    "           print(\"Exit\")\n",
    "           return False\n",
    "    return True\n",
    "\n",
    "# 2a&b (Stemming & exact matches) \n",
    "# NEW: Query Processing Functions\n",
    "\n",
    "# Process the query into a list of tuples (token, exact)\n",
    "def process_query(query):\n",
    "    tokens = []\n",
    "    # this regex finds either \"something in quotes\" or individual words.\n",
    "    pattern = r'\"(.*?)\"|(\\w+)'\n",
    "    for match in re.finditer(pattern, query):\n",
    "        if match.group(1):  # if token is in double quotes => exact match\n",
    "            tokens.append((match.group(1).lower(), True))\n",
    "        elif match.group(2):  # otherwise, token is to be stemmed\n",
    "            tokens.append((match.group(2).lower(), False))\n",
    "    return tokens\n",
    "\n",
    "# rewrite query into a Python expression for eval()\n",
    "def rewrite_query(query, t2i_stem, t2i_exact):\n",
    "    tokens = process_query(query)\n",
    "    token_expressions = []\n",
    "    for token, exact in tokens:\n",
    "        # check for boolean operators (using dictionary d)\n",
    "        if token in d:\n",
    "            token_expressions.append(d[token])\n",
    "        else:\n",
    "            if exact:\n",
    "                # for exact token, use exact index\n",
    "                if token not in t2i_exact:\n",
    "                    token_expressions.append('np.zeros((1, len(documents)), dtype=int)')\n",
    "                else:\n",
    "                    token_expressions.append(f'td_matrix_exact[{t2i_exact[token]}]')\n",
    "            else:\n",
    "                # for non-exact token, stem it and look up in the stem index\n",
    "                stemmed = stemmer.stem(token)\n",
    "                if stemmed not in t2i_stem:\n",
    "                    token_expressions.append('np.zeros((1, len(documents)), dtype=int)')\n",
    "                else:\n",
    "                    token_expressions.append(f'td_matrix_stem[{t2i_stem[stemmed]}]')\n",
    "    return \" \".join(token_expressions)\n",
    "       \n",
    "# check t2i variable: refers to a variable that is only in the main function\n",
    "\n",
    "# OLD: avoid_operators and rewrite_query functions are replaced by the above\n",
    "'''\n",
    "def avoid_operators(t, t2i):\n",
    "   if t in d:\n",
    "       return d[t]\n",
    "   if t not in t2i:\n",
    "       return 'np.zeros((1, len(documents)), dtype=int)'\n",
    "   \n",
    "   return f'td_matrix[{t2i[t]}]'\n",
    "\n",
    "def rewrite_query(query, t2i):\n",
    "   return \" \".join(avoid_operators(t, t2i) for t in query.split())\n",
    "\n",
    "\n",
    "def retrieve_matches(query, documents, td_matrix, t2i):\n",
    "    hits_matrix = eval(rewrite_query(query, t2i))\n",
    "    hits_list = list(hits_matrix.nonzero()[1])\n",
    "    return hits_list\n",
    "'''\n",
    "\n",
    "def retrieve_matches(query, documents, td_matrix_stem, t2i_stem, td_matrix_exact, t2i_exact):\n",
    "    # Create the evaluable query expression\n",
    "    query_expression = rewrite_query(query, t2i_stem, t2i_exact)\n",
    "    try:\n",
    "        # Evaluate the expression. (Note that 'documents' is used in the dummy np.zeros expression.)\n",
    "        hits_matrix = eval(query_expression)\n",
    "    except Exception as e:\n",
    "        print(\"Error evaluating query:\", e)\n",
    "        return []\n",
    "    hits_list = list(hits_matrix.nonzero()[1])\n",
    "    return hits_list\n",
    "        \n",
    "def print_retrieved(hits_list, documents):\n",
    "    if not hits_list:  \n",
    "           print(\"No matching document\")\n",
    "           \n",
    "    else:\n",
    "        print()\n",
    "        print(f\"Found {len(hits_list)} matches:\")\n",
    "        \n",
    "        print_limit = 2 # Determines max number of lines printed\n",
    "        \n",
    "        if len(hits_list) > print_limit:\n",
    "            print(f\"Here are the first {print_limit} results:\")\n",
    "            \n",
    "            e_list = list(enumerate(hits_list))\n",
    "            for i in range(print_limit):\n",
    "                print()\n",
    "                print('%.250s' % \"Matching doc #{:d}: {:s}\".format(e_list[i][0] + 1, documents[e_list[i][1]])) # '%.250s' Number here determines max length of printout per line   \n",
    "                    \n",
    "        else:        \n",
    "            for i, doc_idx in enumerate(hits_list):\n",
    "                print()\n",
    "                print( '%.250s' % \"Matching doc #{:d}: {:s}\".format(i + 1, documents[doc_idx])) # '%.250s' Number here determines max length of printout per line\n",
    "\n",
    "### TF-IDF AND COSINE SIMILARITY FUNCTIONS         \n",
    "# Document setup using TfidfVectorizer\n",
    "def tf_document_setup(documents):\n",
    "    tfv = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\") \n",
    "    tf_matrix = tfv.fit_transform(documents).T.todense() \n",
    "    return tfv, tf_matrix\n",
    "\n",
    "# Compute cosine similarity scores\n",
    "def tf_retrieve_matches(query, tfv, tf_matrix):\n",
    "    query_tf = tfv.transform([query]).todense()  # Convert query to tf-idf vector\n",
    "    scores = np.dot(query_tf, tf_matrix)  # Compute cosine similarity score\n",
    "    return scores\n",
    "\n",
    "#Now works. If you feel like the printout should be something better, feel free to change it.\n",
    "def tf_print_retrieved(scores, documents):\n",
    "    if np.all(scores == 0):  \n",
    "        print(\"No relevant document\")\n",
    "    else:\n",
    "        print()\n",
    "        ranked_scores_and_doc_ids = sorted([(score, doc_idx) for doc_idx, score in enumerate(np.array(scores)[0]) if score > 0], reverse=True) # Rank the documents by similarity score\n",
    "        \n",
    "        print(f\"Found {len(ranked_scores_and_doc_ids)} relevant matches:\")\n",
    "        \n",
    "        print_limit = 2  # Max number of results to display\n",
    "        \n",
    "        if len(ranked_scores_and_doc_ids) > print_limit:\n",
    "            print(f\"Here are the first {print_limit} results, ranked by relevance:\")\n",
    "        \n",
    "            for rank, (score, doc_idx) in enumerate(ranked_scores_and_doc_ids[:print_limit]):\n",
    "                print()\n",
    "                print('%.250s' % f\"Best matching doc #{rank + 1}: {documents[doc_idx]}\") # '%.250s' Number here determines max length of printout per line   \n",
    "                    \n",
    "        else:        \n",
    "            for rank, (score, doc_idx) in enumerate(ranked_scores_and_doc_ids):\n",
    "                print()\n",
    "                print('%.250s' % f\"Best matching doc #{rank + 1}: {documents[doc_idx]}\") # '%.250s' Number here determines max length of printout per line \n",
    "\n",
    "### END OF TF-IDF AND COSINE SIMILARITY FUNCTIONS        \n",
    "\n",
    "def main():\n",
    "    ### DOCUMENTS: Use the documents variable to determine which data you want to use. Please do not change documents variable elsewhere.\n",
    "    documents = extract_wiki_articles(small_wiki) # Assign whatever list of strings you want to use as documents to this variable, comment this line if you don't want to use the small wiki\n",
    "    #documents = extract_wiki_articles(large_wiki) # Assign whatever list of strings you want to use as documents to this variable, comment this line if you don't want to use the large_wiki\n",
    "    # documents = example_documents # uncomment this line to use example_documents\n",
    "    ### END OF DOCUMENTS:\n",
    "    \n",
    "    ### SETUP\n",
    "    # Build two indices: one for stemming (non-exact tokens) and one for exact matches.\n",
    "    td_matrix_stem, t2i_stem = document_setup_stem(documents)\n",
    "    td_matrix_exact, t2i_exact = document_setup_exact(documents)\n",
    "    \n",
    "    # setup for TF-IDF AND COSINE SIMILARITY FUNCTIONS\n",
    "    tfv, tf_matrix  = tf_document_setup(documents)\n",
    "    ### END OF SETUP\n",
    "    \n",
    "    while True:\n",
    "        user_input = user_query()\n",
    "        if input_checker(user_input) == False:\n",
    "            break\n",
    "        \n",
    "        if user_input[:2] == \"-r\":\n",
    "            user_input = user_input[3:]\n",
    "            scores = tf_retrieve_matches(user_input, tfv, tf_matrix)\n",
    "            tf_print_retrieved(scores, documents)\n",
    "        \n",
    "        elif user_input[:2] == \"-n\":  # to activate neural search\n",
    "            user_input = user_input[3:]  \n",
    "            neural_search(documents, user_input)  \n",
    "        \n",
    "        else:  \n",
    "            hits_list = retrieve_matches(user_input, documents,\n",
    "                                          td_matrix_stem, t2i_stem,\n",
    "                                          td_matrix_exact, t2i_exact\n",
    "                                         )\n",
    "            print_retrieved(hits_list, documents)\n",
    "\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "### DATA: I moved these variables here as the program is supposed to work with any data - Liisa\n",
    "\n",
    "    example_documents = [\"This is a silly example\",\n",
    "                \"A better example\",\n",
    "                \"Nothing to see here\",\n",
    "                \"This is a great and long example\"]\n",
    "\n",
    "    ### These paths only work if you run the program from the same directory with the wiki_files folder. \n",
    "    # Akseli: fixed. Now point to parent folder and should be accessible from anywhere.\n",
    "    small_wiki = \"../wiki_files/enwiki-20181001-corpus.100-articles.txt\" \n",
    "    large_wiki = \"../wiki_files/enwiki-20181001-corpus.1000-articles.txt\"\n",
    "\n",
    "### END OF DATA\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48621630-e710-4831-9b18-f81c71890445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
