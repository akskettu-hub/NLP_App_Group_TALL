{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "501398d0-fe72-4e37-bbb7-984bf2dc2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/tkzang/myenv/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (4.48.3)\n",
      "Requirement already satisfied: tqdm in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in /home/tkzang/myenv/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /home/tkzang/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/tkzang/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tkzang/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/tkzang/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tkzang/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tkzang/myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tkzang/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tkzang/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/tkzang/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/tkzang/myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tkzang/myenv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tkzang/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tkzang/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tkzang/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tkzang/myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "## Dependencies for semantic/neural search:\n",
    "import numpy as np\n",
    "!pip install sentence-transformers\n",
    "# We use a pretrained model from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # We can change it to a better model if we find one\n",
    "import json\n",
    "import re # for exact match\n",
    "from nltk.stem import SnowballStemmer # for Finnish stemming\n",
    "stemmer = SnowballStemmer(\"finnish\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41b135d2-c5f1-47b6-bac2-c6c1c7af772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please Enter your query, type 'quit' to exit:  yhtiöjärjestykseen\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your query \"yhtiöjärjestykseen\" matches 197 documents.\n",
      "Here are the top 3 results:\n",
      "\n",
      "Matching doc #1:\n",
      "Title: KKO:2025:13\n",
      "Link: https://www.finlex.fi/fi/oikeus/kko/kko/2025/20250013\n",
      "Diaarinumero: S2023/375\n",
      "Antopäivä: 23.1.2025\n",
      "Description: Osakeyhtiön yhtiökokous päätti ottaa yhtiöjärjestykseen määräyksen siitä, että osakkeenomistaja, jon...\n",
      "(score: 0.43)\n",
      "\n",
      "Matching doc #2:\n",
      "Title: KKO:2024:80\n",
      "Link: https://www.finlex.fi/fi/oikeus/kko/kko/2024/20240080\n",
      "Diaarinumero: S2023/332\n",
      "Antopäivä: 17.12.2024\n",
      "Description: Yhtiö oli tullut osakekaupan seurauksena osaksi konsernia. Yhtiö oli sittemmin ottanut työsuhteeseen...\n",
      "(score: 0.41)\n",
      "\n",
      "Matching doc #3:\n",
      "Title: KKO:2024:69\n",
      "Link: https://www.finlex.fi/fi/oikeus/kko/kko/2024/20240069\n",
      "Diaarinumero: S2023/597\n",
      "Antopäivä: 11.11.2024\n",
      "Description: Lapsen huoltoa ja tapaamisoikeutta koskevan päätöksen täytäntöönpanoasiassa oli järjestetty käräjäoi...\n",
      "(score: 0.40)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please Enter your query, type 'quit' to exit:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit\n"
     ]
    }
   ],
   "source": [
    "def load_documents(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "## updated funtion per Friday's discussion\n",
    "    for year, cases in data.items():\n",
    "        for case_info in cases.values():  \n",
    "            text_content = []\n",
    "            \n",
    "            if \"Title\" in case_info:\n",
    "                text_content.append(f\"Title: {case_info['Title']}\")  \n",
    "            \n",
    "            if \"Metadata\" in case_info:\n",
    "                metadata = case_info[\"Metadata\"]\n",
    "                if \"Link\" in metadata:\n",
    "                    text_content.append(f\"Link: {metadata['Link']}\")\n",
    "                if \"Diaarinumero:\" in metadata:\n",
    "                    text_content.append(f\"Diaarinumero: {metadata['Diaarinumero:']}\")\n",
    "                if \"Antopäivä:\" in metadata:\n",
    "                    text_content.append(f\"Antopäivä: {metadata['Antopäivä:']}\")\n",
    "            \n",
    "            if \"Description\" in case_info:\n",
    "                text_content.append(\"Description:\")\n",
    "                text_content.extend(case_info[\"Description\"])\n",
    "            \n",
    "            ### Suppose we want what's in the \"content\" entries:\n",
    "            '''\n",
    "            for section in [\"Asian käsittely alemmissa oikeuksissa\", \"Muutoksenhaku Korkeimmassa oikeudessa\", \"Korkeimman oikeuden ratkaisu\"]:\n",
    "                if section in case_info and \"Contents\" in case_info[section]:\n",
    "                    text_content.append(f\"\\n{section}:\")\n",
    "                    text_content.extend(case_info[section][\"Contents\"])\n",
    "            '''\n",
    "            \n",
    "            documents.append(\"\\n\".join(text_content))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def neural_search(documents, user_input):\n",
    "    doc_embeddings = model.encode(documents)  # Encode documents\n",
    "    query_embedding = model.encode(user_input)  # Encode user input\n",
    "    \n",
    "    # Calculate cosine similarities and rank documents\n",
    "    cosine_similarities = np.dot(query_embedding, doc_embeddings.T)  \n",
    "    ranked_doc_indices = np.argsort(cosine_similarities)[::-1]  \n",
    "    \n",
    "    num_results = min(3, len(documents))  # Limit to top 3 results\n",
    "    print(f'\\nYour query \"{user_input}\" matches {len(documents)} documents.')\n",
    "    print(f\"Here are the top {num_results} results:\\n\")\n",
    "\n",
    "    result_scores = []\n",
    "    result_titles = []\n",
    "\n",
    "    for i in range(num_results):\n",
    "        doc_idx = ranked_doc_indices[i]\n",
    "        doc_content = documents[doc_idx].split(\"\\n\")\n",
    "        \n",
    "        # Initialize metadata variables\n",
    "        metadata = {key: \"\" for key in [\"Title\", \"Link\", \"Diaarinumero\", \"Antopäivä\"]}\n",
    "        descriptions = []\n",
    "\n",
    "        for line in doc_content:\n",
    "            if line.startswith(\"Title:\"):\n",
    "                metadata[\"Title\"] = line.replace(\"Title:\", \"\").strip()\n",
    "            elif line.startswith(\"Link:\"):\n",
    "                metadata[\"Link\"] = line.replace(\"Link:\", \"\").strip()\n",
    "            elif line.startswith(\"Diaarinumero:\"):\n",
    "                metadata[\"Diaarinumero\"] = line.replace(\"Diaarinumero:\", \"\").strip()\n",
    "            elif line.startswith(\"Antopäivä:\"):\n",
    "                metadata[\"Antopäivä\"] = line.replace(\"Antopäivä:\", \"\").strip()\n",
    "            elif line.startswith(\"Description:\"):\n",
    "                descriptions.extend(doc_content[doc_content.index(line) + 1:])  \n",
    "\n",
    "        description = \" \".join(descriptions)[:100]  ### change the number here to determine the output length\n",
    "\n",
    "        # Store results for plotting\n",
    "        result_scores.append(float(cosine_similarities[doc_idx]))  \n",
    "        result_titles.append(metadata[\"Title\"] if metadata[\"Title\"] else \"Unknown Title\")\n",
    "\n",
    "        # Print formatted result\n",
    "        print(f\"Matching doc #{i+1}:\")\n",
    "        for key in metadata:\n",
    "            if metadata[key]:\n",
    "                print(f\"{key}: {metadata[key]}\")\n",
    "        if description:\n",
    "            print(f\"Description: {description}...\")  \n",
    "        \n",
    "        print(f\"(score: {cosine_similarities[doc_idx]:.2f})\\n\")\n",
    "\n",
    "    return result_scores, result_titles  # Ensure function returns values\n",
    "\n",
    "\n",
    "'''    \n",
    "def plotting(result_scores, result_titles):\n",
    "    # Visualize the results\n",
    "    plt.bar(result_titles, result_scores)\n",
    "    plt.xlabel('Document')\n",
    "    plt.ylabel('Match Score')\n",
    "    plt.title('Top Search Results')\n",
    "    plt.ylim(0.3)\n",
    "    plt.show()\n",
    "'''\n",
    "\n",
    "\n",
    "def user_query():\n",
    "    print()\n",
    "    user_input = input(\"Please Enter your query, type 'quit' to exit: \")\n",
    "    return user_input\n",
    "\n",
    "# tokenizer that applies stemming\n",
    "def stem_tokenizer(text):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())  \n",
    "    return [stemmer.stem(token) for token in tokens]  \n",
    "\n",
    "# Function to process the query (exact match for quoted phrases, stemming for other tokens)\n",
    "def process_query(query):\n",
    "    tokens = []\n",
    "    # This regex finds either \"something in quotes\" or individual words\n",
    "    pattern = r'\"(.*?)\"|(\\w+)'\n",
    "    for match in re.finditer(pattern, query):\n",
    "        if match.group(1):  # If token is in double quotes => exact match\n",
    "            tokens.append((match.group(1).lower(), True))\n",
    "        elif match.group(2):  # Otherwise, token is to be stemmed\n",
    "            tokens.append((match.group(2).lower(), False))\n",
    "    return tokens\n",
    "\n",
    "def input_checker(user_input):\n",
    "    if user_input == \"quit\" or user_input == \"\":\n",
    "        print(\"Exit\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    file_path = 'en_sample_database.json'\n",
    "    documents = load_documents(file_path)\n",
    "\n",
    "    while True:\n",
    "        user_input = user_query()\n",
    "        if input_checker(user_input) == False: \n",
    "            break\n",
    "            \n",
    "        result_scores, result_titles = neural_search(documents, user_input)\n",
    "        '''plotting(result_scores, result_titles)'''\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e8550-cdd5-4d51-9039-9bb7bb4e311e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
