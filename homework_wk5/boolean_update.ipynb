{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aad02ce1-ab38-4dea-bc90-5da7e5234cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # for tfidf functions later\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "from nltk.stem import SnowballStemmer # for Finnish stemming\n",
    "stemmer = SnowballStemmer(\"finnish\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a393d0-8c18-4ef1-93c8-ff13e6326b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_documents(file_path):\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "## This loop works on the sample database, but need to be modified if the keys change as the dadtabase develops\n",
    "## The metadata and link parts are left out but can be added if needed\n",
    "    for year, cases in data.items():\n",
    "        for case_info in cases.values():  \n",
    "            text_content = []\n",
    "            \n",
    "            if \"Title\" in case_info:\n",
    "                text_content.append(\"Title:\")  \n",
    "                text_content.append(case_info[\"Title\"])\n",
    "            \n",
    "            if \"Description\" in case_info:\n",
    "                text_content.append(\"Description:\")\n",
    "                text_content.extend(case_info[\"Description\"])\n",
    "            \n",
    "            for section in [\"Asian kÃ¤sittely alemmissa oikeuksissa\", \"Muutoksenhaku Korkeimmassa oikeudessa\", \"Korkeimman oikeuden ratkaisu\"]:\n",
    "                if section in case_info and \"Contents\" in case_info[section]:\n",
    "                    text_content.append(f\"\\n{section}:\")\n",
    "                    text_content.extend(case_info[section][\"Contents\"])\n",
    "            \n",
    "            documents.append(\"\\n\".join(text_content))\n",
    "\n",
    "    return documents\n",
    "\n",
    "d = {\"and\": \"&\", \"AND\": \"&\",\n",
    "     \"or\": \"|\", \"OR\": \"|\",\n",
    "     \"not\": \"1 -\", \"NOT\": \"1 -\",\n",
    "     \"(\": \"(\", \")\": \")\"}          # operator replacements\n",
    "\n",
    "# 4 The most likely reason why not all words are indexed is the default token pattern used by CountVectorizer: r'\\b\\w\\w+\\b' This pattern only matches words with two or more alphanumeric characters. See changed token_pattern when initalizing CountVectorizer at the top of the page\n",
    "def document_setup(documents):\n",
    "    cv = CountVectorizer(lowercase=True, binary=True, token_pattern=r'\\b\\w+\\b') ### changed token_pattern as part of homework #4\n",
    "    sparse_matrix = cv.fit_transform(documents)\n",
    "    dense_matrix = sparse_matrix.todense()\n",
    "    td_matrix = dense_matrix.T\n",
    "    t2i = cv.vocabulary_ \n",
    "    \n",
    "    return (td_matrix, t2i)\n",
    "\n",
    "\n",
    "\n",
    "def user_query():\n",
    "    print()\n",
    "    user_input = input(\"Please Enter your query, type 'quit' to exit: \")\n",
    "    print()\n",
    "    return user_input\n",
    "\n",
    "#Modification of former rewrite_token() from course material that handles words not in documents\n",
    "def avoid_operators(t):\n",
    "   if t in d:\n",
    "       return d[t]\n",
    "   if t not in t2i:\n",
    "       return 'np.zeros((1, len(documents)), dtype=int)'\n",
    "   \n",
    "   return f'td_matrix[{t2i[t]}]'\n",
    "\n",
    "def rewrite_query(query):\n",
    "   return \" \".join(avoid_operators(t) for t in query.split())\n",
    "\n",
    "def input_checker(user_input):\n",
    "    if user_input == \"quit\" or user_input == \"\":\n",
    "           print(\"Exit\")\n",
    "           return False\n",
    "        \n",
    "\n",
    "\n",
    "def stemming(documents):\n",
    "    \n",
    "    stemmed_documents = []\n",
    "    for doc in documents:\n",
    "        tokens = word_tokenize(doc)  # Tokenize the document\n",
    "        stemmed_tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming to each token\n",
    "        stemmed_documents.append(\" \".join(stemmed_tokens))  # Join the tokens back into a document\n",
    "    \n",
    "    return stemmed_documents\n",
    "\n",
    "def input_checker(user_input):\n",
    "    if user_input == \"quit\" or user_input == \"\":\n",
    "        print(\"Exit\")\n",
    "        return False\n",
    "    return True                \n",
    "import re\n",
    "\n",
    "# This is new. The old one for exact match doesn't work on this script\n",
    "def exact_match(query, documents):   \n",
    "    pattern = re.compile(r'\\b' + query + r'\\b', re.IGNORECASE) # match the exact query as a whole, with t\n",
    "    \n",
    "    matching_docs = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        if pattern.search(doc):\n",
    "            matching_docs.append(i)\n",
    "    \n",
    "    return matching_docs\n",
    "\n",
    "# Modify the retrieve_matches function to use exact match\n",
    "def retrieve_matches(query):\n",
    "    # Check if the query begins and ends with \" \"\n",
    "    if query.startswith('\"') and query.endswith('\"'):\n",
    "        # Remove quotes and perform exact match search\n",
    "        query = query[1:-1]\n",
    "        return exact_match(query, documents)\n",
    "    \n",
    "    # Otherwise proceed with original query rewrite and operator processing\n",
    "    hits_matrix = eval(rewrite_query(query))\n",
    "    hits_list = list(hits_matrix.nonzero()[1])\n",
    "    return hits_list\n",
    "\n",
    "        \n",
    "def print_retrieved(hits_list):\n",
    "    if not hits_list:  \n",
    "           print(\"No matching document\")\n",
    "           \n",
    "    else:\n",
    "        print(f\"Found {len(hits_list)} matches:\")\n",
    "        \n",
    "        print_limit = 2 # Determines max number of lines printed\n",
    "        \n",
    "        if len(hits_list) > print_limit:\n",
    "            print(f\"Here are the first {print_limit} results:\")\n",
    "            \n",
    "            e_list = list(enumerate(hits_list))\n",
    "            for i in range(print_limit):\n",
    "                print()\n",
    "                print('%.250s' % \"Matching doc #{:d}: {:s}\".format(e_list[i][0] + 1, documents[e_list[i][1]])) # '%.250s' Number here determines max length of printout per line\n",
    "                \n",
    "                    \n",
    "        else:        \n",
    "            for i, doc_idx in enumerate(hits_list):\n",
    "                print()\n",
    "                print( '%.250s' % \"Matching doc #{:d}: {:s}\".format(i + 1, documents[doc_idx])) # '%.250s' Number here determines max length of printout per line\n",
    "            \n",
    "def main():\n",
    "    file_path = 'sample_database.json'\n",
    "    documents = load_documents(file_path)\n",
    "    documents = documents\n",
    "    setup = document_setup(documents) \n",
    "    td_matrix = setup[0]\n",
    "    t2i = setup[1]\n",
    "    while True:\n",
    "        user_input = user_query()\n",
    "        if input_checker(user_input) == False:\n",
    "            break\n",
    "        hits_list = retrieve_matches(user_input)\n",
    "        print_retrieved(hits_list)\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'sample_database.json'\n",
    "    documents = load_documents(file_path)\n",
    "    documents = documents\n",
    "    setup = document_setup(documents) \n",
    "    td_matrix = setup[0]\n",
    "    t2i = setup[1]\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c21f77-ea42-49fd-97f1-03d39f288f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
